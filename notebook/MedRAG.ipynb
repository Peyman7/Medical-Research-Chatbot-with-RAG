{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index openai langchain langchain_community langchain-openai faiss-cpu arxiv serpapi google-search-results requests beautifulsoup4 streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gBxfu2p9Um"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "serpapi_key = userdata.get('SERPAPI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpCSMy6NnItx"
      },
      "source": [
        "**Fetching Research Papers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULinL8HUO-FQ"
      },
      "source": [
        "1. Fetching Papers from arXiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjuGWwqYPHhv"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "\n",
        "def fetch_arxiv_papers(query, max_results=5):\n",
        "  \"Fetch latest research papers from arXiv based on query.\"\n",
        "  client = arxiv.Client()\n",
        "  search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "  results = client.results(search)\n",
        "  papers = []\n",
        "  for result in results:\n",
        "    papers.append({\n",
        "          \"title\": result.title,\n",
        "          \"summary\": result.summary,\n",
        "          \"source\": \"arXiv\"\n",
        "      })\n",
        "\n",
        "  return papers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLry3AS7Pin_"
      },
      "source": [
        "2. Fetching Papers from PubMed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5_JW0TlnMxc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_pubmed_abstract(pmid):\n",
        "    \"\"\"Fetch abstract from PubMed using PubMed ID (PMID).\"\"\"\n",
        "    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    abstraction_section = soup.find(\"div\", class_=\"abstract-content\")\n",
        "    if abstraction_section:\n",
        "      return abstraction_section.text.strip()\n",
        "    else:\n",
        "      return \"Abstract not found\"\n",
        "\n",
        "def fetch_pubmed_papers(query, max_results=5):\n",
        "    \"\"\"Fetch medical research papers from PubMed.\"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "    search_url = base_url + \"esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmode\": \"json\",\n",
        "        \"retmax\": max_results\n",
        "    }\n",
        "\n",
        "    response = requests.get(search_url, params=params).json()\n",
        "    pmid_list = response['esearchresult']['idlist']\n",
        "\n",
        "    # Fetch abstract for each pmid\n",
        "    papers = []\n",
        "    for pmid in pmid_list:\n",
        "      abstract = get_pubmed_abstract(pmid)\n",
        "      papers.append({\n",
        "          \"title\": f\"PumEd article {pmid}\",\n",
        "          \"summary\": abstract,\n",
        "          \"source\": \"PubMed\"\n",
        "      })\n",
        "\n",
        "    return papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaZ4btM6R8t7"
      },
      "source": [
        "3. Fetching Papers from Google Scholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5YgRQklSCDu"
      },
      "outputs": [],
      "source": [
        "from serpapi import GoogleSearch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def get_scholar_details(scholar_url):\n",
        "  \"\"\"Extracts title and abstract from a Google Scholar article page.\"\"\"\n",
        "  response = requests.get(scholar_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  title_element = soup.find(\"h3\", class_=\"gs_rt\")\n",
        "  abstract_name = soup.find(\"div\", class_=\"gs_rs\")\n",
        "\n",
        "  return {\n",
        "      \"title\": title_element.text.strip() if title_element else \"Title not found\",\n",
        "      \"summary\": abstract_name.text.strip() if abstract_name else \"Abstract not found\"\n",
        "  }\n",
        "\n",
        "\n",
        "def search_google_scholar(query, max_results=5):\n",
        "    \"Fetch research papers from Google Scholar based on query.\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": serpapi_key,\n",
        "        \"engine\": \"google_scholar\",\n",
        "        \"num\": max_results\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict().get(\"organic_results\", [])\n",
        "\n",
        "    papers = []\n",
        "    for result in results:\n",
        "      url = result.get(\"link\")\n",
        "      details = get_scholar_details(url) if url else {\"title\": result[\"title\"], \"abstract\": \"No abstract available\"}\n",
        "\n",
        "      papers.append({\n",
        "          \"title\": details[\"title\"],\n",
        "          \"summary\": details[\"summary\"],\n",
        "          \"source\": \"Google Scholar\"\n",
        "      })\n",
        "\n",
        "\n",
        "    return papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m31ieF0ZVfUU"
      },
      "source": [
        "**Process Papers for Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1K6hsbLVlGX"
      },
      "outputs": [],
      "source": [
        "#from llama_index.core import Document\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "def process_papers(papers, max_words=100):\n",
        "  \"\"\"Processes research papers for embedding into the chatbot's memory.\"\"\"\n",
        "  documents = []\n",
        "  for paper in papers:\n",
        "    doc_text = f\"Title: {paper['title']}\\nAbstract: {paper['summary']}\\nSource: {paper['source']}\"\n",
        "    doc_text = \" \".join(doc_text.split()[:max_words])  # Truncate to max_words words\n",
        "    # The Document class in LangChain expects 'page_content' instead of 'text'\n",
        "    documents.append(Document(page_content=doc_text, metadata={\"title\": paper[\"title\"], \"source\": paper[\"source\"]}))\n",
        "\n",
        "  return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WK6SVmtW7mI"
      },
      "source": [
        "**Summarization Using LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwMRW8J7XAio"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "def summarize_paper(paper_summary, max_retries=3):\n",
        "    \"\"\"Generate a summary using OpenAI GPT-4 with error handling.\"\"\"\n",
        "    prompt = f\"You are a specialized assistant for summarizing medical and healthcare papers.Summarize the following text in maximum 3 sentence:\\n\\n{paper_summary}\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=50  # Adjust token limit\n",
        "            )\n",
        "            return response.choices[0].message.content  # Extract the generated summary\n",
        "\n",
        "        except openai.RateLimitError as e:\n",
        "            print(f\"Rate limit exceeded. Attempt {attempt+1}/{max_retries}. Retrying in 10 seconds...\")\n",
        "            time.sleep(10)  # Wait before retrying\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return \"Error in summarization\"\n",
        "\n",
        "    return \"Failed after multiple retries\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxrZ9aVxXk4S"
      },
      "source": [
        "**Build RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwCJ1WtXXohY"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "#from llama_index.embedding.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "def build_rag_pipeline(documents):\n",
        "  \"\"\"Builds a RAG-based chatbot with retrieved documents.\"\"\"\n",
        "  service_context = ServiceContext.from_defaults(\n",
        "      embed_model = OpenAIEmbedding(embed_batch_size=10),\n",
        "  )\n",
        "  index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttiSU6AdYq0s"
      },
      "source": [
        "**Create Chatbot with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3YavySlYw6u"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
        "\n",
        "def create_chatbot(documents):\n",
        "  \"\"\"Creates a chatbot agent using LangChain.\"\"\"\n",
        "  vectorstore = FAISS.from_documents(documents, OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  llm = ChatOpenAI(temperature=0.0, model_name=\"gpt-4o\", openai_api_key=openai_api_key)\n",
        "  chatbot = ConversationalRetrievalChain.from_llm(\n",
        "      llm = llm,\n",
        "      retriever = retriever,\n",
        "      memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10, input_key='question', memory_key=\"chat_history\", return_docs=False, return_messages=True),\n",
        "  )\n",
        "\n",
        "  return chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPo-K-wzCRZF"
      },
      "source": [
        "**Inference in Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em5X-IY5uwJr"
      },
      "outputs": [],
      "source": [
        "query = \"Sleep role in memory consolidation\"\n",
        "papers = fetch_arxiv_papers(query, max_results=1) + fetch_pubmed_papers(query, max_results=1) + search_google_scholar(query, max_results=1)\n",
        "documents = process_papers(papers)\n",
        "chatbot = create_chatbot(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "WRbNo-Opf9kF",
        "outputId": "d4cbdb11-4f45-42f0-84e2-2de3fc6b1f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🗨️ Ask a question about these papers (or type 'exit' to quit): How brain memory consolidates during sleep?\n",
            "\n",
            "🤖 Chatbot Response:\n",
            " {'question': 'How brain memory consolidates during sleep?', 'chat_history': [], 'answer': 'Memory consolidation during sleep involves the strengthening and stabilization of memories that were acquired during wakefulness. This process is believed to occur through the reactivation and reorganization of neural circuits. During sleep, particularly during rapid-eye-movement (REM) sleep, specific brain regions, such as the hippocampus and cortex, are active in replaying and processing information. This activity helps to integrate new memories with existing knowledge, making them more stable and less susceptible to interference. Additionally, certain neurotransmitters and hormones, like oxytocin, may play a role in facilitating memory consolidation, as suggested by research on the prelimbic cortex and its involvement in social memory consolidation.'}\n",
            "\n",
            "🗨️ Ask a question about these papers (or type 'exit' to quit): what is episodic memory?\n",
            "\n",
            "🤖 Chatbot Response:\n",
            " {'question': 'what is episodic memory?', 'chat_history': [SystemMessage(content=\"The human asks how brain memory consolidates during sleep. The AI explains that memory consolidation involves the strengthening and stabilization of memories acquired during wakefulness, occurring through the reactivation and reorganization of neural circuits. During REM sleep, brain regions like the hippocampus and cortex replay and process information, integrating new memories with existing knowledge. Neurotransmitters and hormones, such as oxytocin, may also facilitate this process, as indicated by research on the prelimbic cortex's role in social memory consolidation.\", additional_kwargs={}, response_metadata={})], 'answer': 'Episodic memory is a type of long-term memory that involves the recollection of specific events, situations, and experiences. It allows individuals to remember personal experiences and specific events in time, often with contextual details such as the location, time, and emotions associated with the event. Episodic memory is a component of declarative memory, which also includes semantic memory, the memory of facts and general knowledge.'}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9a84f616d054>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run chatbot interaction in a loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🗨️ Ask a question about these papers (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n👋 Exiting chatbot. Have a great day!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "q = \"How brain memory consolidates during sleep?\"\n",
        "chat_history = []\n",
        "# Run chatbot interaction in a loop\n",
        "while True:\n",
        "      user_input = input(\"\\n🗨️ Ask a question about these papers (or type 'exit' to quit): \")\n",
        "      if user_input.lower() == \"exit\":\n",
        "          print(\"\\n👋 Exiting chatbot. Have a great day!\")\n",
        "          break\n",
        "\n",
        "      #response = chatbot.invoke({\"question\": user_input, \"chat_history\": chat_history})\n",
        "      response = chatbot.invoke({\"question\": user_input, \"chat_history\": chat_history})\n",
        "      chat_history.append((user_input, response[\"answer\"]))\n",
        "      print(\"\\n🤖 Chatbot Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQnz94LZCUmf"
      },
      "source": [
        "**Deployment in Streamlit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kHnkLvxC2pE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv_ag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
